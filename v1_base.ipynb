{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v1 - dev",
      "provenance": [],
      "collapsed_sections": [
        "UnPgLO-a2L5V",
        "ycS7yK0y2Izc"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUjRW3AY+kS//Ld9QYIlDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedefliguer/trading/blob/master/v1_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpyze2fnDD08"
      },
      "source": [
        "## Instalación librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlDuK1EHQx4j",
        "outputId": "b1c939d9-e117-49c1-922f-74e947b9ec4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22616 sha256=5bab45693c8c55e7ca1ba7d1804e31a4b5c222e289883302f2d685fb58593a59\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SWGVvJZCmYl"
      },
      "source": [
        "## Detalle de funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfpAx_G0TbjQ"
      },
      "source": [
        "import yfinance as yf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import xgboost as xgb\r\n",
        "from xgboost.sklearn import XGBClassifier\r\n",
        "pd.options.mode.chained_assignment = None\r\n",
        "from datetime import date\r\n",
        "from datetime import timedelta \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.stats as st\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.ensemble import RandomForestClassifier \r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import fbeta_score\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "pd.set_option('display.max_columns', 400)\r\n",
        "pd.set_option('display.max_rows', 5000)\r\n",
        "pd.set_option('display.width', 1000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQYnsTOCTlZ3"
      },
      "source": [
        "def descarga(ticker, fc_empieza, fc_termina):\r\n",
        "  base = yf.download(ticker, start=fc_empieza, end=fc_termina)\r\n",
        "  base = base[['Close', 'Volume', 'High', 'Low']]\r\n",
        "  base.insert(loc=0, column='Ticker', value=ticker)\r\n",
        "  base.reset_index(level=0, inplace=True)\r\n",
        "  base.columns=['fc', 'ticker', 'y', 'vl', 'high', 'low']\r\n",
        "  return base"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oClKKS0zWr9f"
      },
      "source": [
        "def calcula_pc_merval(dataset):\r\n",
        "  dataset = pd.merge(dataset,mvl,on='fc',how='left')\r\n",
        "  dataset['pc_merval'] = dataset.y/dataset.mvl\r\n",
        "  dataset = dataset.drop(['mvl'], axis=1)\r\n",
        "  return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uulgHlR-Xe2o"
      },
      "source": [
        "def calcula_amplitud(dataset):\r\n",
        "  dataset['amplitud'] = (dataset.high - dataset.low)/dataset.y\r\n",
        "  return dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBXvhAzXXzsW"
      },
      "source": [
        "def estandariza_volumen(dataset):\r\n",
        "  mean_vl = dataset['vl'].mean()\r\n",
        "  std_vl = dataset['vl'].std()\r\n",
        "  dataset['vl'] = (dataset.vl - mean_vl)/std_vl\r\n",
        "  return dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DclkiKuTzvG5"
      },
      "source": [
        "def calcula_medias(dataset):\r\n",
        "  period = 12\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp1'] = ema\r\n",
        "  period = 26\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp2'] = ema\r\n",
        "  macd = dataset['exp1']-dataset['exp2']\r\n",
        "  dataset['macd'] = macd\r\n",
        "  dataset['exp3'] = macd.ewm(span=9, adjust=False).mean()\r\n",
        "  dataset['histog'] = dataset['macd'] - dataset['exp3'] \r\n",
        "  return dataset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0RVcwNiYW-n"
      },
      "source": [
        "def calcula_historia(dataset, lags):\r\n",
        "  for (columnName, columnData) in dataset.iloc[:,6:].iteritems():\r\n",
        "    i = 1\r\n",
        "    while i < lags:\r\n",
        "      colname = \"var_%s_%s\" % (columnName, i)\r\n",
        "      dataset[colname] = columnData/columnData.shift(i)-1\r\n",
        "      i = i + 1\r\n",
        "  return dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUhyPKRL3AKn"
      },
      "source": [
        "def calcula_canalidad_y(dataset):\r\n",
        "  i = 1\r\n",
        "  dataset['lag_y_1'] = dataset.y.shift(1)\r\n",
        "  dataset['nu_dias_y_entre_max_min_30'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_30'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_90'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_90'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_180'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_180'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset = dataset.drop(['lag_y_1'], axis=1)\r\n",
        "  i = 2\r\n",
        "  while i < 30:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_30'] = dataset['nu_dias_y_entre_max_min_30'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_30'] = dataset['nu_dias_y_entre_5pc_30'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 90:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_90'] = dataset['nu_dias_y_entre_max_min_90'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_90'] = dataset['nu_dias_y_entre_5pc_90'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 180:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_180'] = dataset['nu_dias_y_entre_max_min_180'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_180'] = dataset['nu_dias_y_entre_5pc_180'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  return dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryqPlgcv5Vd1"
      },
      "source": [
        "def calcula_canalidad_histog_macd(dataset):\r\n",
        "  list = [5, 30, 90, 180]\r\n",
        "  for ventana in list:\r\n",
        "    i = 1\r\n",
        "    dataset['lag_histog_1'] = dataset.histog.shift(1)\r\n",
        "    colname_nu_1 = \"nu_dias_histog_entre_5pc_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_1] = np.where((dataset['lag_histog_1'] < (dataset.histog * 1.05)) & (dataset['lag_histog_1'] > (dataset.histog * 0.95)), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_2 = \"nu_dias_histog_positivo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_2] = np.where((dataset['lag_histog_1']>0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_3 = \"nu_dias_histog_negativo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_3] = np.where((dataset['lag_histog_1']<0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_4 = \"nu_dias_histog_mismo_signo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_4] = np.where(((dataset['lag_histog_1']>0) & (dataset['histog']>0))|((dataset['lag_histog_1']<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "\r\n",
        "    dataset = dataset.drop(['lag_histog_1'], axis=1)\r\n",
        "    i = 2\r\n",
        "    while i < (ventana+1):\r\n",
        "      colname = \"lag_histog_%s\" % (i)\r\n",
        "      dataset[colname] = dataset.histog.shift(i)\r\n",
        "      dataset[colname_nu_1] = dataset[colname_nu_1] + np.where((dataset[colname] < (dataset.histog * 1.50)) & (dataset[colname] > (dataset.histog * 0.50)), 1, 0)\r\n",
        "      dataset[colname_nu_2] = dataset[colname_nu_2] + np.where((dataset[colname]>0), 1, 0)\r\n",
        "      dataset[colname_nu_3] = dataset[colname_nu_3] + np.where((dataset[colname]<0), 1, 0)\r\n",
        "      dataset[colname_nu_4] = dataset[colname_nu_4] + np.where(((dataset[colname]>0) & (dataset['histog']>0))|((dataset[colname]<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "      i = i + 1\r\n",
        "      dataset = dataset.drop([colname], axis=1)\r\n",
        "  return dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fky3qlEu_KUU"
      },
      "source": [
        "def calcula_AT_tendencias(dataset, lags):\r\n",
        "  \r\n",
        "  # Construye las columnas para determinar si es un pico\r\n",
        "  i = 1\r\n",
        "  while i < (lags+1):\r\n",
        "      colname = 'p%sb' % (i)                                                  \r\n",
        "      dataset[colname] = round(dataset.y.shift(i),2)\r\n",
        "      j = i * -1\r\n",
        "      colname = 'p%sf' % (-j)                                                  \r\n",
        "      dataset[colname] = round(dataset.y.shift(j),2)\r\n",
        "      i = i + 1\r\n",
        "\r\n",
        "  # Determina si es un pico  \r\n",
        "  dataset['maxb'] = round(dataset.filter(regex=(\".*b\")).max(axis=1),2)\r\n",
        "  dataset['maxf']= round(dataset.filter(regex=(\".*f\")).max(axis=1),2)\r\n",
        "  dataset['minb'] = round(dataset.filter(regex=(\".*b\")).min(axis=1),2)\r\n",
        "  dataset['minf'] = round(dataset.filter(regex=(\".*f\")).min(axis=1),2)\r\n",
        "  dataset['T'] = np.where((dataset['y']>dataset['maxb']) & (dataset['y']>dataset['maxf']), 1, 0)\r\n",
        "  dataset['P'] = np.where((dataset['y']<dataset['minb']) & (dataset['y']<dataset['minf']), 1, 0)\r\n",
        "\r\n",
        "  techos = dataset[(dataset['T']==1)]\r\n",
        "  techos['m'] = (techos.y.shift(1) - techos.y)/(techos.fc.shift(1) - techos.fc).dt.days\r\n",
        "  techos.name = 'techos'\r\n",
        "  pisos = dataset[(dataset['P']==1)]\r\n",
        "  pisos['m'] = (pisos.y.shift(1) - pisos.y)/(pisos.fc.shift(1) - pisos.fc).dt.days\r\n",
        "  pisos.name = 'pisos'\r\n",
        "  dataset_list = [techos, pisos]\r\n",
        "\r\n",
        "  for dataset_picos in dataset_list:  # En cada dataset (techos y pisos)\r\n",
        "    name = dataset_picos.name\r\n",
        "    dias = len(dataset)\r\n",
        "    for index, row in dataset_picos.iloc[1:].iterrows(): # Para cada pico detectado (fila del dataset) a partir del segundo (porque el primero no tiene anterior, no tiene tendencia)\r\n",
        "      y_start = row['y']\r\n",
        "      pendiente = row['m']\r\n",
        "      if (dias < np.where(dataset.fc==row['fc'])[0] + lags):\r\n",
        "        continue    \r\n",
        "      serie = [] # Crea la serie que va a contener el precio proyectado\r\n",
        "      serie = np.append(serie, np.repeat(np.nan, (np.where(dataset.fc==row['fc'])[0] + lags))) # Appendea nulos hasta el día en el que confirmamos que nació una tendencia\r\n",
        "      i = np.where(dataset.fc==row['fc'])[0] + lags\r\n",
        "      while (i < dias):\r\n",
        "        dia = i - (np.where(dataset.fc==row['fc'])[0] + lags)\r\n",
        "        serie = np.append(serie, (y_start + pendiente*lags) + pendiente*dia)\r\n",
        "        i = i + 1 # Appendea el precio proyectado hasta el final\r\n",
        "\r\n",
        "      colname = '%s_%s_proy' % (name, index)  # Precio proyectado\r\n",
        "      dataset[colname] = serie # Construye la columna de toda la serie\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue superado\r\n",
        "      colname_pass = '%s_%s_pass' % (name, index) # Pico pasado\r\n",
        "      if name == 'techos':\r\n",
        "        dataset[colname_pass] = np.where(dataset['y']>(dataset[colname])*1.005, 1, 0)\r\n",
        "      elif name == 'pisos':\r\n",
        "        dataset[colname_pass] = np.where(dataset['y']<(dataset[colname])*0.995, 1, 0)\r\n",
        "      dataset[colname_pass] = dataset[colname_pass].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue probado\r\n",
        "      colname_prueba = '%s_%s_prueba' % (name, index)  \r\n",
        "      dataset[colname_prueba] = np.where((dataset['y']>dataset[colname]*0.995)&(dataset['y']<dataset[colname]*1.005), 1, 0)\r\n",
        "      dataset[colname_prueba] = dataset[colname_prueba].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con pendiente del pico\r\n",
        "      colname_pendiente = '%s_%s_pendiente' % (name, index)  \r\n",
        "      dataset[colname_pendiente] = row['m']\r\n",
        "\r\n",
        "      # Creo la combinacion y elimino cada uno\r\n",
        "      colname_comb = '%s_%s' % (name, index)\r\n",
        "      dataset[colname_comb] = dataset[[colname, colname_pass, colname_prueba, colname_pendiente]].values.tolist()\r\n",
        "      del dataset[colname]\r\n",
        "      del dataset[colname_pass]\r\n",
        "      del dataset[colname_prueba]\r\n",
        "      del dataset[colname_pendiente]\r\n",
        "\r\n",
        "  # Creo el objeto por cada techo o piso individual\r\n",
        "  names_techos = dataset.filter(regex=(\"(techos)(.*)\")).columns\r\n",
        "  names_pisos = dataset.filter(regex=(\"(pisos)(.*)\")).columns\r\n",
        "\r\n",
        "  for index, row in dataset.iterrows():  # Por cada fila del dataset original (por cada precio)\r\n",
        "\r\n",
        "    # Genero las rows vacías con las variables agregadas\r\n",
        "    nu_pruebas_techo_vivo_mas_probado = np.nan    \r\n",
        "    precio_proyectado_techo_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_techo_vivo_mas_cercano = np.nan\r\n",
        "    precio_proyectado_techo_muerto_mas_cercano = np.nan\r\n",
        "    tendencia_techo_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "    nu_pruebas_piso_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_piso_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_piso_vivo_mas_cercano = np.nan\r\n",
        "    precio_proyectado_piso_muerto_mas_cercano = np.nan\r\n",
        "    tendencia_piso_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "    # Voy a recorrer cada tendencia proyectada para definir cuáles van, en caso de que corresponda lo asigno a estas variables agregadas\r\n",
        "\r\n",
        "    i = 0\r\n",
        "    while i < len(row.index): # Por cada uno de los picos de los que se puede armar tendencia\r\n",
        "      if (row.index[i] in names_techos):  # Si es un techo\r\n",
        "        if row[i][1]>5: # Si está muerto\r\n",
        "          if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_muerto_mas_cercano) or np.isnan(precio_proyectado_techo_muerto_mas_cercano): # Si está muerto y proyecta precio más cercano que el actual\r\n",
        "            precio_proyectado_techo_muerto_mas_cercano = row[i][0]\r\n",
        "            \r\n",
        "        else: # Si está vivo\r\n",
        "          if row[i][2] > nu_pruebas_techo_vivo_mas_probado or (np.isnan(nu_pruebas_techo_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "            nu_pruebas_techo_vivo_mas_probado = row[i][2]\r\n",
        "            precio_proyectado_techo_vivo_mas_probado = row[i][0]\r\n",
        "            tendencia_techo_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "          if (np.isnan(precio_proyectado_techo_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "            precio_proyectado_techo_vivo_mas_cercano = row[i][0]\r\n",
        "\r\n",
        "      elif (row.index[i] in names_pisos):\r\n",
        "        if row[i][1]>5: # Si está muerto\r\n",
        "          if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_muerto_mas_cercano) or np.isnan(precio_proyectado_piso_muerto_mas_cercano): # Si proyecta precio más cercano que el actual\r\n",
        "            precio_proyectado_piso_muerto_mas_cercano = row[i][0]\r\n",
        "            \r\n",
        "        else: # Si está vivo\r\n",
        "          if row[i][2] > nu_pruebas_piso_vivo_mas_probado or (np.isnan(nu_pruebas_piso_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "            nu_pruebas_piso_vivo_mas_probado = row[i][2]\r\n",
        "            precio_proyectado_piso_vivo_mas_probado = row[i][0]\r\n",
        "            tendencia_piso_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "          if (np.isnan(precio_proyectado_piso_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "            precio_proyectado_piso_vivo_mas_cercano = row[i][0]\r\n",
        "      i = i + 1\r\n",
        "        \r\n",
        "    dataset.loc[index,'nu_pruebas_techo_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_techo_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_techo_vivo_mas_probado_'f\"{lags}\"] = tendencia_techo_vivo_mas_probado/row['y']\r\n",
        "\r\n",
        "    dataset.loc[index,'nu_pruebas_piso_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_piso_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_piso_vivo_mas_probado_'f\"{lags}\"] = tendencia_piso_vivo_mas_probado/row['y']\r\n",
        "\r\n",
        "  # Elimino todas las que construí excepto estas\r\n",
        "  i = 1\r\n",
        "  while i < (lags+1):\r\n",
        "      colname = 'p%sb' % (i)                                                  \r\n",
        "      dataset = dataset.drop(colname, axis=1)\r\n",
        "      j = i * -1\r\n",
        "      colname = 'p%sf' % (-j)                                                  \r\n",
        "      dataset = dataset.drop(colname, axis=1)\r\n",
        "      i = i + 1\r\n",
        "\r\n",
        "  ultimas_drop = ['maxb', 'maxf', 'minb', 'minf', 'T', 'P']\r\n",
        "  dataset = dataset.drop(ultimas_drop, axis=1)\r\n",
        "  dataset = dataset.drop(names_techos, axis=1)\r\n",
        "  dataset = dataset.drop(names_pisos, axis=1)\r\n",
        "  return dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1p5ImqhvrrW"
      },
      "source": [
        "def calcula_target_class(dataset, SL, TG, dias_indeterminacion):\r\n",
        "  dataset['target'] = 99\r\n",
        "  i = 1\r\n",
        "  while i <= dias_indeterminacion:\r\n",
        "    var_y_low = dataset.low.shift(-i)/df.y-1 # Variación del mínimo de cada día contra el precio de compra\r\n",
        "    var_y_high = dataset.high.shift(-i)/df.y-1 # Variación del máximo de cada día contra el precio de compra\r\n",
        "    target = np.where(var_y_low < -SL, 0, 99)\r\n",
        "    target = np.where(var_y_high > TG, 1, target)\r\n",
        "    dataset['target'] = np.where(dataset['target'] == 99 , target , dataset['target'])\r\n",
        "    i = i + 1\r\n",
        "#  df = df.iloc[:-dias_indeterminacion]  # Elimino las últimas filas que no llegan a tener target\r\n",
        "  return dataset"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU49lNHBZjif"
      },
      "source": [
        "def divide_dev_test(dataset, start_train, start_test, end_test):\r\n",
        "  global df, x_dev, y_dev, x_test, y_test, x_val, y_val, df_test, df_dev\r\n",
        "  month = dataset['fc'].dt.strftime('%Y%m')\r\n",
        "  month = pd.to_numeric(month)\r\n",
        "  if 'month' not in dataset:\r\n",
        "    dataset.insert (1, \"month\", month)\r\n",
        "  dataset = dataset[(dataset.target) < 90] # Elimina indeterminados\r\n",
        "\r\n",
        "  df_dev = dataset[(dataset.month >= start_train) & (dataset.month < start_test)]\r\n",
        "  df_test = dataset[(dataset.month >= start_test) & (dataset.month <= end_test)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdZ3VEj-Ch-M"
      },
      "source": [
        "## Consolidado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljjf2-cEUzC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e994c38-8809-4792-bd21-266c723a2299"
      },
      "source": [
        "dias_empieza = 5000\r\n",
        "dias_termina = 200\r\n",
        "today = date.today()\r\n",
        "fc_empieza = today + timedelta(days=(dias_empieza*-1))\r\n",
        "fc_termina = today + timedelta(days=(dias_termina*-1))\r\n",
        "\r\n",
        "mvl = yf.download('^MERV', start=fc_empieza, end=fc_termina)\r\n",
        "print(\"Descargado Merval\")\r\n",
        "mvl = mvl[['Close']]\r\n",
        "mvl.reset_index(level=0, inplace=True)\r\n",
        "mvl.columns=['fc','mvl']\r\n",
        "base = pd.DataFrame()\r\n",
        "\r\n",
        "for ticker in (\r\n",
        "    'GGAL.BA',\r\n",
        "    'BMA.BA',\r\n",
        "    'BYMA.BA',\r\n",
        "    'CEPU.BA',\r\n",
        "    'COME.BA',\r\n",
        "    'CRES.BA',\r\n",
        "    'CVH.BA',\r\n",
        "    'EDN.BA',\r\n",
        "    'MIRG.BA',\r\n",
        "    'PAMP.BA',\r\n",
        "    'SUPV.BA',\r\n",
        "    'TECO2.BA',\r\n",
        "    'TGNO4.BA',\r\n",
        "    'TGSU2.BA',\r\n",
        "    'TRAN.BA',\r\n",
        "    'VALO.BA',\r\n",
        "    'YPFD.BA'\r\n",
        "):\r\n",
        "  df = descarga(ticker, fc_empieza, fc_termina) # (Días empieza, días termina)\r\n",
        "  print(\"Descargado \", ticker)\r\n",
        "  df = calcula_pc_merval(df)\r\n",
        "  df = calcula_amplitud(df)\r\n",
        "  df = estandariza_volumen(df)\r\n",
        " \r\n",
        "  df = calcula_medias(df)\r\n",
        "  df = df.dropna()\r\n",
        "  df = calcula_historia(df, 5) # (Lags)\r\n",
        "  df = calcula_canalidad_y(df)\r\n",
        "  df = calcula_canalidad_histog_macd(df)\r\n",
        "\r\n",
        "  for per in (360, 120, 90, 60, 30, 15, 8, 4):\r\n",
        "    df = calcula_AT_tendencias(df,per)\r\n",
        "    print(\"Calculé AT para\", ticker, \"en lags de\", per)\r\n",
        "  \r\n",
        "  df = calcula_target_class(df, 0.06, 0.14, 90) # (Stop loss, Take gain, Días para indeterminación)\r\n",
        "  base = base.append(df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "Descargado Merval\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  GGAL.BA\n",
            "Calculé AT para GGAL.BA en lags de 360\n",
            "Calculé AT para GGAL.BA en lags de 120\n",
            "Calculé AT para GGAL.BA en lags de 90\n",
            "Calculé AT para GGAL.BA en lags de 60\n",
            "Calculé AT para GGAL.BA en lags de 30\n",
            "Calculé AT para GGAL.BA en lags de 15\n",
            "Calculé AT para GGAL.BA en lags de 8\n",
            "Calculé AT para GGAL.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  BMA.BA\n",
            "Calculé AT para BMA.BA en lags de 360\n",
            "Calculé AT para BMA.BA en lags de 120\n",
            "Calculé AT para BMA.BA en lags de 90\n",
            "Calculé AT para BMA.BA en lags de 60\n",
            "Calculé AT para BMA.BA en lags de 30\n",
            "Calculé AT para BMA.BA en lags de 15\n",
            "Calculé AT para BMA.BA en lags de 8\n",
            "Calculé AT para BMA.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  BYMA.BA\n",
            "Calculé AT para BYMA.BA en lags de 360\n",
            "Calculé AT para BYMA.BA en lags de 120\n",
            "Calculé AT para BYMA.BA en lags de 90\n",
            "Calculé AT para BYMA.BA en lags de 60\n",
            "Calculé AT para BYMA.BA en lags de 30\n",
            "Calculé AT para BYMA.BA en lags de 15\n",
            "Calculé AT para BYMA.BA en lags de 8\n",
            "Calculé AT para BYMA.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  CEPU.BA\n",
            "Calculé AT para CEPU.BA en lags de 360\n",
            "Calculé AT para CEPU.BA en lags de 120\n",
            "Calculé AT para CEPU.BA en lags de 90\n",
            "Calculé AT para CEPU.BA en lags de 60\n",
            "Calculé AT para CEPU.BA en lags de 30\n",
            "Calculé AT para CEPU.BA en lags de 15\n",
            "Calculé AT para CEPU.BA en lags de 8\n",
            "Calculé AT para CEPU.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  COME.BA\n",
            "Calculé AT para COME.BA en lags de 360\n",
            "Calculé AT para COME.BA en lags de 120\n",
            "Calculé AT para COME.BA en lags de 90\n",
            "Calculé AT para COME.BA en lags de 60\n",
            "Calculé AT para COME.BA en lags de 30\n",
            "Calculé AT para COME.BA en lags de 15\n",
            "Calculé AT para COME.BA en lags de 8\n",
            "Calculé AT para COME.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  CRES.BA\n",
            "Calculé AT para CRES.BA en lags de 360\n",
            "Calculé AT para CRES.BA en lags de 120\n",
            "Calculé AT para CRES.BA en lags de 90\n",
            "Calculé AT para CRES.BA en lags de 60\n",
            "Calculé AT para CRES.BA en lags de 30\n",
            "Calculé AT para CRES.BA en lags de 15\n",
            "Calculé AT para CRES.BA en lags de 8\n",
            "Calculé AT para CRES.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  CVH.BA\n",
            "Calculé AT para CVH.BA en lags de 360\n",
            "Calculé AT para CVH.BA en lags de 120\n",
            "Calculé AT para CVH.BA en lags de 90\n",
            "Calculé AT para CVH.BA en lags de 60\n",
            "Calculé AT para CVH.BA en lags de 30\n",
            "Calculé AT para CVH.BA en lags de 15\n",
            "Calculé AT para CVH.BA en lags de 8\n",
            "Calculé AT para CVH.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  EDN.BA\n",
            "Calculé AT para EDN.BA en lags de 360\n",
            "Calculé AT para EDN.BA en lags de 120\n",
            "Calculé AT para EDN.BA en lags de 90\n",
            "Calculé AT para EDN.BA en lags de 60\n",
            "Calculé AT para EDN.BA en lags de 30\n",
            "Calculé AT para EDN.BA en lags de 15\n",
            "Calculé AT para EDN.BA en lags de 8\n",
            "Calculé AT para EDN.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  MIRG.BA\n",
            "Calculé AT para MIRG.BA en lags de 360\n",
            "Calculé AT para MIRG.BA en lags de 120\n",
            "Calculé AT para MIRG.BA en lags de 90\n",
            "Calculé AT para MIRG.BA en lags de 60\n",
            "Calculé AT para MIRG.BA en lags de 30\n",
            "Calculé AT para MIRG.BA en lags de 15\n",
            "Calculé AT para MIRG.BA en lags de 8\n",
            "Calculé AT para MIRG.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  PAMP.BA\n",
            "Calculé AT para PAMP.BA en lags de 360\n",
            "Calculé AT para PAMP.BA en lags de 120\n",
            "Calculé AT para PAMP.BA en lags de 90\n",
            "Calculé AT para PAMP.BA en lags de 60\n",
            "Calculé AT para PAMP.BA en lags de 30\n",
            "Calculé AT para PAMP.BA en lags de 15\n",
            "Calculé AT para PAMP.BA en lags de 8\n",
            "Calculé AT para PAMP.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  SUPV.BA\n",
            "Calculé AT para SUPV.BA en lags de 360\n",
            "Calculé AT para SUPV.BA en lags de 120\n",
            "Calculé AT para SUPV.BA en lags de 90\n",
            "Calculé AT para SUPV.BA en lags de 60\n",
            "Calculé AT para SUPV.BA en lags de 30\n",
            "Calculé AT para SUPV.BA en lags de 15\n",
            "Calculé AT para SUPV.BA en lags de 8\n",
            "Calculé AT para SUPV.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  TECO2.BA\n",
            "Calculé AT para TECO2.BA en lags de 360\n",
            "Calculé AT para TECO2.BA en lags de 120\n",
            "Calculé AT para TECO2.BA en lags de 90\n",
            "Calculé AT para TECO2.BA en lags de 60\n",
            "Calculé AT para TECO2.BA en lags de 30\n",
            "Calculé AT para TECO2.BA en lags de 15\n",
            "Calculé AT para TECO2.BA en lags de 8\n",
            "Calculé AT para TECO2.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  TGNO4.BA\n",
            "Calculé AT para TGNO4.BA en lags de 360\n",
            "Calculé AT para TGNO4.BA en lags de 120\n",
            "Calculé AT para TGNO4.BA en lags de 90\n",
            "Calculé AT para TGNO4.BA en lags de 60\n",
            "Calculé AT para TGNO4.BA en lags de 30\n",
            "Calculé AT para TGNO4.BA en lags de 15\n",
            "Calculé AT para TGNO4.BA en lags de 8\n",
            "Calculé AT para TGNO4.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  TGSU2.BA\n",
            "Calculé AT para TGSU2.BA en lags de 360\n",
            "Calculé AT para TGSU2.BA en lags de 120\n",
            "Calculé AT para TGSU2.BA en lags de 90\n",
            "Calculé AT para TGSU2.BA en lags de 60\n",
            "Calculé AT para TGSU2.BA en lags de 30\n",
            "Calculé AT para TGSU2.BA en lags de 15\n",
            "Calculé AT para TGSU2.BA en lags de 8\n",
            "Calculé AT para TGSU2.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  TRAN.BA\n",
            "Calculé AT para TRAN.BA en lags de 360\n",
            "Calculé AT para TRAN.BA en lags de 120\n",
            "Calculé AT para TRAN.BA en lags de 90\n",
            "Calculé AT para TRAN.BA en lags de 60\n",
            "Calculé AT para TRAN.BA en lags de 30\n",
            "Calculé AT para TRAN.BA en lags de 15\n",
            "Calculé AT para TRAN.BA en lags de 8\n",
            "Calculé AT para TRAN.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  VALO.BA\n",
            "Calculé AT para VALO.BA en lags de 360\n",
            "Calculé AT para VALO.BA en lags de 120\n",
            "Calculé AT para VALO.BA en lags de 90\n",
            "Calculé AT para VALO.BA en lags de 60\n",
            "Calculé AT para VALO.BA en lags de 30\n",
            "Calculé AT para VALO.BA en lags de 15\n",
            "Calculé AT para VALO.BA en lags de 8\n",
            "Calculé AT para VALO.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  YPFD.BA\n",
            "Calculé AT para YPFD.BA en lags de 360\n",
            "Calculé AT para YPFD.BA en lags de 120\n",
            "Calculé AT para YPFD.BA en lags de 90\n",
            "Calculé AT para YPFD.BA en lags de 60\n",
            "Calculé AT para YPFD.BA en lags de 30\n",
            "Calculé AT para YPFD.BA en lags de 15\n",
            "Calculé AT para YPFD.BA en lags de 8\n",
            "Calculé AT para YPFD.BA en lags de 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpl2C5gldWEX",
        "outputId": "f1531be6-fd81-4471-eb2e-058f02777df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        }
      },
      "source": [
        "from google.colab import files\r\n",
        "base.to_csv('v1.csv')\r\n",
        "files.download('v1.csv')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_724f3209-0cfe-4410-8ff0-3f589aaa2bbb\", \"v1.csv\", 69146290)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}