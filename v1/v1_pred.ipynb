{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v1 - pred",
      "provenance": [],
      "collapsed_sections": [
        "UnPgLO-a2L5V",
        "ycS7yK0y2Izc"
      ],
      "authorship_tag": "ABX9TyOxgymBFEOEAqRdbm0wDTEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedefliguer/trading/blob/master/v1_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIJ3BQybWd6r"
      },
      "source": [
        "!pip install yfinance\r\n",
        "\r\n",
        "import yfinance as yf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import xgboost as xgb\r\n",
        "from xgboost.sklearn import XGBClassifier\r\n",
        "pd.options.mode.chained_assignment = None\r\n",
        "from datetime import date\r\n",
        "from datetime import timedelta \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.stats as st\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.ensemble import RandomForestClassifier \r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import fbeta_score\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "pd.set_option('display.max_columns', 400)\r\n",
        "pd.set_option('display.max_rows', 5000)\r\n",
        "pd.set_option('display.width', 1000)\r\n",
        "\r\n",
        "def descarga(ticker, fc_empieza, fc_termina):\r\n",
        "  base = yf.download(ticker, start=fc_empieza, end=fc_termina)\r\n",
        "  base = base[['Close', 'Volume', 'High', 'Low']]\r\n",
        "  base.insert(loc=0, column='Ticker', value=ticker)\r\n",
        "  base.reset_index(level=0, inplace=True)\r\n",
        "  base.columns=['fc', 'ticker', 'y', 'vl', 'high', 'low']\r\n",
        "  return base\r\n",
        "\r\n",
        "def calcula_pc_merval(dataset):\r\n",
        "  dataset = pd.merge(dataset,mvl,on='fc',how='left')\r\n",
        "  dataset['pc_merval'] = dataset.y/dataset.mvl\r\n",
        "  dataset = dataset.drop(['mvl'], axis=1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_amplitud(dataset):\r\n",
        "  dataset['amplitud'] = (dataset.high - dataset.low)/dataset.y\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def estandariza_volumen(dataset):\r\n",
        "  mean_vl = dataset['vl'].mean()\r\n",
        "  std_vl = dataset['vl'].std()\r\n",
        "  dataset['vl'] = (dataset.vl - mean_vl)/std_vl\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_medias(dataset):\r\n",
        "  period = 12\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp1'] = ema\r\n",
        "  period = 26\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp2'] = ema\r\n",
        "  macd = dataset['exp1']-dataset['exp2']\r\n",
        "  dataset['macd'] = macd\r\n",
        "  dataset['exp3'] = macd.ewm(span=9, adjust=False).mean()\r\n",
        "  dataset['histog'] = dataset['macd'] - dataset['exp3'] \r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_historia(dataset, lags):\r\n",
        "  for (columnName, columnData) in dataset.iloc[:,6:].iteritems():\r\n",
        "    i = 1\r\n",
        "    while i < lags:\r\n",
        "      colname = \"var_%s_%s\" % (columnName, i)\r\n",
        "      dataset[colname] = columnData/columnData.shift(i)-1\r\n",
        "      i = i + 1\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_canalidad_y(dataset):\r\n",
        "  i = 1\r\n",
        "  dataset['lag_y_1'] = dataset.y.shift(1)\r\n",
        "  dataset['nu_dias_y_entre_max_min_30'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_30'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_90'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_90'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_180'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_180'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset = dataset.drop(['lag_y_1'], axis=1)\r\n",
        "  i = 2\r\n",
        "  while i < 30:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_30'] = dataset['nu_dias_y_entre_max_min_30'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_30'] = dataset['nu_dias_y_entre_5pc_30'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 90:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_90'] = dataset['nu_dias_y_entre_max_min_90'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_90'] = dataset['nu_dias_y_entre_5pc_90'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 180:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_180'] = dataset['nu_dias_y_entre_max_min_180'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_180'] = dataset['nu_dias_y_entre_5pc_180'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_canalidad_histog_macd(dataset):\r\n",
        "  list = [5, 30, 90, 180]\r\n",
        "  for ventana in list:\r\n",
        "    i = 1\r\n",
        "    dataset['lag_histog_1'] = dataset.histog.shift(1)\r\n",
        "    colname_nu_1 = \"nu_dias_histog_entre_5pc_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_1] = np.where((dataset['lag_histog_1'] < (dataset.histog * 1.05)) & (dataset['lag_histog_1'] > (dataset.histog * 0.95)), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_2 = \"nu_dias_histog_positivo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_2] = np.where((dataset['lag_histog_1']>0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_3 = \"nu_dias_histog_negativo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_3] = np.where((dataset['lag_histog_1']<0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_4 = \"nu_dias_histog_mismo_signo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_4] = np.where(((dataset['lag_histog_1']>0) & (dataset['histog']>0))|((dataset['lag_histog_1']<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "\r\n",
        "    dataset = dataset.drop(['lag_histog_1'], axis=1)\r\n",
        "    i = 2\r\n",
        "    while i < (ventana+1):\r\n",
        "      colname = \"lag_histog_%s\" % (i)\r\n",
        "      dataset[colname] = dataset.histog.shift(i)\r\n",
        "      dataset[colname_nu_1] = dataset[colname_nu_1] + np.where((dataset[colname] < (dataset.histog * 1.50)) & (dataset[colname] > (dataset.histog * 0.50)), 1, 0)\r\n",
        "      dataset[colname_nu_2] = dataset[colname_nu_2] + np.where((dataset[colname]>0), 1, 0)\r\n",
        "      dataset[colname_nu_3] = dataset[colname_nu_3] + np.where((dataset[colname]<0), 1, 0)\r\n",
        "      dataset[colname_nu_4] = dataset[colname_nu_4] + np.where(((dataset[colname]>0) & (dataset['histog']>0))|((dataset[colname]<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "      i = i + 1\r\n",
        "      dataset = dataset.drop([colname], axis=1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_AT_tendencias(dataset, lags):\r\n",
        "  \r\n",
        "  # Construye las columnas para determinar si es un pico\r\n",
        "  i = 1\r\n",
        "  while i < (lags+1):\r\n",
        "      colname = 'p%sb' % (i)                                                  \r\n",
        "      dataset[colname] = round(dataset.y.shift(i),2)\r\n",
        "      j = i * -1\r\n",
        "      colname = 'p%sf' % (-j)                                                  \r\n",
        "      dataset[colname] = round(dataset.y.shift(j),2)\r\n",
        "      i = i + 1\r\n",
        "\r\n",
        "  # Determina si es un pico  \r\n",
        "  dataset['maxb'] = round(dataset.filter(regex=(\".*b\")).max(axis=1),2)\r\n",
        "  dataset['maxf']= round(dataset.filter(regex=(\".*f\")).max(axis=1),2)\r\n",
        "  dataset['minb'] = round(dataset.filter(regex=(\".*b\")).min(axis=1),2)\r\n",
        "  dataset['minf'] = round(dataset.filter(regex=(\".*f\")).min(axis=1),2)\r\n",
        "  dataset['T'] = np.where((dataset['y']>dataset['maxb']) & (dataset['y']>dataset['maxf']), 1, 0)\r\n",
        "  dataset['P'] = np.where((dataset['y']<dataset['minb']) & (dataset['y']<dataset['minf']), 1, 0)\r\n",
        "\r\n",
        "  techos = dataset[(dataset['T']==1)]\r\n",
        "  techos['m'] = (techos.y.shift(1) - techos.y)/(techos.fc.shift(1) - techos.fc).dt.days\r\n",
        "  techos.name = 'techos'\r\n",
        "  pisos = dataset[(dataset['P']==1)]\r\n",
        "  pisos['m'] = (pisos.y.shift(1) - pisos.y)/(pisos.fc.shift(1) - pisos.fc).dt.days\r\n",
        "  pisos.name = 'pisos'\r\n",
        "  dataset_list = [techos, pisos]\r\n",
        "\r\n",
        "  for dataset_picos in dataset_list:  # En cada dataset (techos y pisos)\r\n",
        "    name = dataset_picos.name\r\n",
        "    dias = len(dataset)\r\n",
        "    for index, row in dataset_picos.iloc[1:].iterrows(): # Para cada pico detectado (fila del dataset) a partir del segundo (porque el primero no tiene anterior, no tiene tendencia)\r\n",
        "      y_start = row['y']\r\n",
        "      pendiente = row['m']\r\n",
        "      if (dias < np.where(dataset.fc==row['fc'])[0] + lags):\r\n",
        "        continue    \r\n",
        "      serie = [] # Crea la serie que va a contener el precio proyectado\r\n",
        "      serie = np.append(serie, np.repeat(np.nan, (np.where(dataset.fc==row['fc'])[0] + lags))) # Appendea nulos hasta el día en el que confirmamos que nació una tendencia\r\n",
        "      i = np.where(dataset.fc==row['fc'])[0] + lags\r\n",
        "      while (i < dias):\r\n",
        "        dia = i - (np.where(dataset.fc==row['fc'])[0] + lags)\r\n",
        "        serie = np.append(serie, (y_start + pendiente*lags) + pendiente*dia)\r\n",
        "        i = i + 1 # Appendea el precio proyectado hasta el final\r\n",
        "\r\n",
        "      colname = '%s_%s_proy' % (name, index)  # Precio proyectado\r\n",
        "      dataset[colname] = serie # Construye la columna de toda la serie\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue superado\r\n",
        "      colname_pass = '%s_%s_pass' % (name, index) # Pico pasado\r\n",
        "      if name == 'techos':\r\n",
        "        dataset[colname_pass] = np.where(dataset['y']>(dataset[colname])*1.005, 1, 0)\r\n",
        "      elif name == 'pisos':\r\n",
        "        dataset[colname_pass] = np.where(dataset['y']<(dataset[colname])*0.995, 1, 0)\r\n",
        "      dataset[colname_pass] = dataset[colname_pass].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue probado\r\n",
        "      colname_prueba = '%s_%s_prueba' % (name, index)  \r\n",
        "      dataset[colname_prueba] = np.where((dataset['y']>dataset[colname]*0.995)&(dataset['y']<dataset[colname]*1.005), 1, 0)\r\n",
        "      dataset[colname_prueba] = dataset[colname_prueba].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con pendiente del pico\r\n",
        "      colname_pendiente = '%s_%s_pendiente' % (name, index)  \r\n",
        "      dataset[colname_pendiente] = row['m']\r\n",
        "\r\n",
        "      # Creo la combinacion y elimino cada uno\r\n",
        "      colname_comb = '%s_%s' % (name, index)\r\n",
        "      dataset[colname_comb] = dataset[[colname, colname_pass, colname_prueba, colname_pendiente]].values.tolist()\r\n",
        "      del dataset[colname]\r\n",
        "      del dataset[colname_pass]\r\n",
        "      del dataset[colname_prueba]\r\n",
        "      del dataset[colname_pendiente]\r\n",
        "\r\n",
        "  # Creo el objeto por cada techo o piso individual\r\n",
        "  names_techos = dataset.filter(regex=(\"(techos)(.*)\")).columns\r\n",
        "  names_pisos = dataset.filter(regex=(\"(pisos)(.*)\")).columns\r\n",
        "\r\n",
        "  for index, row in dataset.iterrows():  # Por cada fila del dataset original (por cada precio)\r\n",
        "\r\n",
        "    # Genero las rows vacías con las variables agregadas\r\n",
        "    nu_pruebas_techo_vivo_mas_probado = np.nan    \r\n",
        "    precio_proyectado_techo_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_techo_vivo_mas_cercano = np.nan\r\n",
        "    precio_proyectado_techo_muerto_mas_cercano = np.nan\r\n",
        "    tendencia_techo_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "    nu_pruebas_piso_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_piso_vivo_mas_probado = np.nan\r\n",
        "    precio_proyectado_piso_vivo_mas_cercano = np.nan\r\n",
        "    precio_proyectado_piso_muerto_mas_cercano = np.nan\r\n",
        "    tendencia_piso_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "    # Voy a recorrer cada tendencia proyectada para definir cuáles van, en caso de que corresponda lo asigno a estas variables agregadas\r\n",
        "\r\n",
        "    i = 0\r\n",
        "    while i < len(row.index): # Por cada uno de los picos de los que se puede armar tendencia\r\n",
        "      if (row.index[i] in names_techos):  # Si es un techo\r\n",
        "        if row[i][1]>5: # Si está muerto\r\n",
        "          if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_muerto_mas_cercano) or np.isnan(precio_proyectado_techo_muerto_mas_cercano): # Si está muerto y proyecta precio más cercano que el actual\r\n",
        "            precio_proyectado_techo_muerto_mas_cercano = row[i][0]\r\n",
        "            \r\n",
        "        else: # Si está vivo\r\n",
        "          if row[i][2] > nu_pruebas_techo_vivo_mas_probado or (np.isnan(nu_pruebas_techo_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "            nu_pruebas_techo_vivo_mas_probado = row[i][2]\r\n",
        "            precio_proyectado_techo_vivo_mas_probado = row[i][0]\r\n",
        "            tendencia_techo_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "          if (np.isnan(precio_proyectado_techo_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "            precio_proyectado_techo_vivo_mas_cercano = row[i][0]\r\n",
        "\r\n",
        "      elif (row.index[i] in names_pisos):\r\n",
        "        if row[i][1]>5: # Si está muerto\r\n",
        "          if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_muerto_mas_cercano) or np.isnan(precio_proyectado_piso_muerto_mas_cercano): # Si proyecta precio más cercano que el actual\r\n",
        "            precio_proyectado_piso_muerto_mas_cercano = row[i][0]\r\n",
        "            \r\n",
        "        else: # Si está vivo\r\n",
        "          if row[i][2] > nu_pruebas_piso_vivo_mas_probado or (np.isnan(nu_pruebas_piso_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "            nu_pruebas_piso_vivo_mas_probado = row[i][2]\r\n",
        "            precio_proyectado_piso_vivo_mas_probado = row[i][0]\r\n",
        "            tendencia_piso_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "          if (np.isnan(precio_proyectado_piso_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "            precio_proyectado_piso_vivo_mas_cercano = row[i][0]\r\n",
        "      i = i + 1\r\n",
        "        \r\n",
        "    dataset.loc[index,'nu_pruebas_techo_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_techo_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_techo_vivo_mas_probado_'f\"{lags}\"] = tendencia_techo_vivo_mas_probado/row['y']\r\n",
        "\r\n",
        "    dataset.loc[index,'nu_pruebas_piso_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_piso_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_piso_vivo_mas_probado_'f\"{lags}\"] = tendencia_piso_vivo_mas_probado/row['y']\r\n",
        "\r\n",
        "  # Elimino todas las que construí excepto estas\r\n",
        "  i = 1\r\n",
        "  while i < (lags+1):\r\n",
        "      colname = 'p%sb' % (i)                                                  \r\n",
        "      dataset = dataset.drop(colname, axis=1)\r\n",
        "      j = i * -1\r\n",
        "      colname = 'p%sf' % (-j)                                                  \r\n",
        "      dataset = dataset.drop(colname, axis=1)\r\n",
        "      i = i + 1\r\n",
        "\r\n",
        "  ultimas_drop = ['maxb', 'maxf', 'minb', 'minf', 'T', 'P']\r\n",
        "  dataset = dataset.drop(ultimas_drop, axis=1)\r\n",
        "  dataset = dataset.drop(names_techos, axis=1)\r\n",
        "  dataset = dataset.drop(names_pisos, axis=1)\r\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "E3MBSDRVWKUS",
        "outputId": "bd3b3938-4389-434c-a13c-8430926b80c4"
      },
      "source": [
        "today = date.today()\r\n",
        "fc_empieza = today + timedelta(days=-5000)\r\n",
        "fc_termina = today + timedelta(days=2)\r\n",
        "\r\n",
        "mvl = yf.download('^MERV', start=fc_empieza, end=fc_termina)\r\n",
        "print(\"Descargado Merval\")\r\n",
        "mvl = mvl[['Close']]\r\n",
        "mvl.reset_index(level=0, inplace=True)\r\n",
        "mvl.columns=['fc','mvl']\r\n",
        "base = pd.DataFrame()\r\n",
        "\r\n",
        "for ticker in (\r\n",
        "    'GGAL.BA',\r\n",
        "    'BMA.BA',\r\n",
        "    'BYMA.BA',\r\n",
        "    'CEPU.BA',\r\n",
        "    'COME.BA',\r\n",
        "    'CRES.BA',\r\n",
        "    'CVH.BA',\r\n",
        "    'EDN.BA',\r\n",
        "    'MIRG.BA',\r\n",
        "    'PAMP.BA',\r\n",
        "    'SUPV.BA',\r\n",
        "    'TECO2.BA',\r\n",
        "    'TGNO4.BA',\r\n",
        "    'TGSU2.BA',\r\n",
        "    'TRAN.BA',\r\n",
        "    'VALO.BA',\r\n",
        "    'YPFD.BA'\r\n",
        "):\r\n",
        "  df = descarga(ticker, fc_empieza, fc_termina) # (Días empieza, días termina)\r\n",
        "  print(\"Descargado \", ticker)\r\n",
        "  \r\n",
        "  df = calcula_pc_merval(df)\r\n",
        "  df = calcula_amplitud(df)\r\n",
        "  df = estandariza_volumen(df)\r\n",
        " \r\n",
        "  df = calcula_medias(df)\r\n",
        "  df = calcula_historia(df, 5) # (Lags)\r\n",
        "  df = calcula_canalidad_y(df)\r\n",
        "  df = calcula_canalidad_histog_macd(df)\r\n",
        "\r\n",
        "  for per in (360, 120, 90, 60, 30, 15, 8, 4):\r\n",
        "    df = calcula_AT_tendencias(df,per)\r\n",
        "    print(\"Calculé AT para\", ticker, \"en lags de\", per)\r\n",
        "  \r\n",
        "  df = df.tail(1)\r\n",
        "  base = base.append(df)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado Merval\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  GGAL.BA\n",
            "Calculé AT para GGAL.BA en lags de 360\n",
            "Calculé AT para GGAL.BA en lags de 120\n",
            "Calculé AT para GGAL.BA en lags de 90\n",
            "Calculé AT para GGAL.BA en lags de 60\n",
            "Calculé AT para GGAL.BA en lags de 30\n",
            "Calculé AT para GGAL.BA en lags de 15\n",
            "Calculé AT para GGAL.BA en lags de 8\n",
            "Calculé AT para GGAL.BA en lags de 4\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Descargado  BMA.BA\n",
            "Calculé AT para BMA.BA en lags de 360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5125\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5126\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-7b70f711ab42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcula_AT_tendencias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculé AT para\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en lags de\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0e1e297e313c>\u001b[0m in \u001b[0;36mcalcula_AT_tendencias\u001b[0;34m(dataset, lags)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mdia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mserie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpendiente\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpendiente\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Appendea el precio proyectado hasta el final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "idhK9HlWXeD0",
        "outputId": "db1608e6-9fe1-456d-d432-73dbd06888ae"
      },
      "source": [
        "from io import BytesIO\r\n",
        "import pickle\r\n",
        "import requests\r\n",
        "mLink = 'https://github.com/fedefliguer/trading/blob/master/v1_model.dat?raw=true'\r\n",
        "mfile = BytesIO(requests.get(mLink).content)\r\n",
        "model = pickle.load(mfile)\r\n",
        "\r\n",
        "X=base.iloc[:, 6:]\r\n",
        "preds = model.predict_proba(X)[:,1]\r\n",
        "prediccions = base.iloc[:, 0:3]\r\n",
        "prediccions['pred'] = preds\r\n",
        "prediccions.sort_values(by=['pred'], ascending=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fc</th>\n",
              "      <th>ticker</th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>2021-01-26</td>\n",
              "      <td>GGAL.BA</td>\n",
              "      <td>117.800003</td>\n",
              "      <td>0.626842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             fc   ticker           y      pred\n",
              "3328 2021-01-26  GGAL.BA  117.800003  0.626842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}