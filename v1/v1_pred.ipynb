{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v1 - pred",
      "provenance": [],
      "collapsed_sections": [
        "UnPgLO-a2L5V",
        "ycS7yK0y2Izc"
      ],
      "authorship_tag": "ABX9TyO6Yef99zavmJ8jy8KEKrrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedefliguer/trading/blob/master/v1/v1_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIJ3BQybWd6r"
      },
      "source": [
        "!pip install yfinance\r\n",
        "\r\n",
        "import yfinance as yf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import xgboost as xgb\r\n",
        "from xgboost.sklearn import XGBClassifier\r\n",
        "pd.options.mode.chained_assignment = None\r\n",
        "from datetime import date\r\n",
        "from datetime import timedelta \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.stats as st\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.ensemble import RandomForestClassifier \r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import fbeta_score\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from io import BytesIO\r\n",
        "import pickle\r\n",
        "import requests\r\n",
        "\r\n",
        "pd.set_option('display.max_columns', 400)\r\n",
        "pd.set_option('display.max_rows', 5000)\r\n",
        "pd.set_option('display.width', 1000)\r\n",
        "\r\n",
        "def descarga(ticker, fc_empieza, fc_termina):\r\n",
        "  base = yf.download(ticker, start=fc_empieza, end=fc_termina)\r\n",
        "  base = base[['Close', 'Volume', 'High', 'Low']]\r\n",
        "  base.insert(loc=0, column='Ticker', value=ticker)\r\n",
        "  base.reset_index(level=0, inplace=True)\r\n",
        "  base.columns=['fc', 'ticker', 'y', 'vl', 'high', 'low']\r\n",
        "  return base\r\n",
        "\r\n",
        "def calcula_pc_merval(dataset):\r\n",
        "  dataset = pd.merge(dataset,mvl,on='fc',how='left')\r\n",
        "  dataset['pc_merval'] = dataset.y/dataset.mvl\r\n",
        "  dataset = dataset.drop(['mvl'], axis=1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_amplitud(dataset):\r\n",
        "  dataset['amplitud'] = (dataset.high - dataset.low)/dataset.y\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def estandariza_volumen(dataset):\r\n",
        "  mean_vl = dataset['vl'].mean()\r\n",
        "  std_vl = dataset['vl'].std()\r\n",
        "  dataset['vl'] = (dataset.vl - mean_vl)/std_vl\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_medias(dataset):\r\n",
        "  period = 12\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp1'] = ema\r\n",
        "  period = 26\r\n",
        "  sma = dataset['y'].rolling(period, min_periods=period).mean()\r\n",
        "  idx_start = sma.isna().sum() + 1 - period\r\n",
        "  idx_end = idx_start + period\r\n",
        "  sma = sma[idx_start: idx_end]\r\n",
        "  rest = dataset['y'][idx_end:]\r\n",
        "  ema = pd.concat([sma, rest]).ewm(span=period, adjust=False).mean()\r\n",
        "  dataset['exp2'] = ema\r\n",
        "  macd = dataset['exp1']-dataset['exp2']\r\n",
        "  dataset['macd'] = macd\r\n",
        "  dataset['exp3'] = macd.ewm(span=9, adjust=False).mean()\r\n",
        "  dataset['histog'] = dataset['macd'] - dataset['exp3'] \r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_historia(dataset, lags):\r\n",
        "  for (columnName, columnData) in dataset.iloc[:,6:].iteritems():\r\n",
        "    i = 1\r\n",
        "    while i < lags:\r\n",
        "      colname = \"var_%s_%s\" % (columnName, i)\r\n",
        "      dataset[colname] = columnData/columnData.shift(i)-1\r\n",
        "      i = i + 1\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_canalidad_y(dataset):\r\n",
        "  i = 1\r\n",
        "  dataset['lag_y_1'] = dataset.y.shift(1)\r\n",
        "  dataset['nu_dias_y_entre_max_min_30'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_30'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_90'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_90'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset['nu_dias_y_entre_max_min_180'] = np.where((dataset['lag_y_1'] < dataset['high']) & (dataset['lag_y_1'] > dataset['low']), 1, 0)\r\n",
        "  dataset['nu_dias_y_entre_5pc_180'] = np.where((dataset['lag_y_1'] < (dataset.y * 1.05)) & (dataset['lag_y_1'] > (dataset.y * 0.95)), 1, 0)\r\n",
        "\r\n",
        "  dataset = dataset.drop(['lag_y_1'], axis=1)\r\n",
        "  i = 2\r\n",
        "  while i < 30:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_30'] = dataset['nu_dias_y_entre_max_min_30'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_30'] = dataset['nu_dias_y_entre_5pc_30'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 90:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_90'] = dataset['nu_dias_y_entre_max_min_90'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_90'] = dataset['nu_dias_y_entre_5pc_90'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  i = 2\r\n",
        "  while i < 180:\r\n",
        "    colname = \"lag_y_%s\" % (i)\r\n",
        "    dataset[colname] = dataset.y.shift(i)\r\n",
        "    dataset['nu_dias_y_entre_max_min_180'] = dataset['nu_dias_y_entre_max_min_180'] + np.where((dataset[colname] < dataset['high']) & (dataset[colname] > dataset['low']), 1, 0)\r\n",
        "    dataset['nu_dias_y_entre_5pc_180'] = dataset['nu_dias_y_entre_5pc_180'] + np.where((dataset[colname] < (dataset.y * 1.05)) & (dataset[colname] > (dataset.y * 0.95)), 1, 0)\r\n",
        "    i = i + 1\r\n",
        "    dataset = dataset.drop([colname], axis=1)\r\n",
        "\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_canalidad_histog_macd(dataset):\r\n",
        "  list = [5, 30, 90, 180]\r\n",
        "  for ventana in list:\r\n",
        "    i = 1\r\n",
        "    dataset['lag_histog_1'] = dataset.histog.shift(1)\r\n",
        "    colname_nu_1 = \"nu_dias_histog_entre_5pc_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_1] = np.where((dataset['lag_histog_1'] < (dataset.histog * 1.05)) & (dataset['lag_histog_1'] > (dataset.histog * 0.95)), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_2 = \"nu_dias_histog_positivo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_2] = np.where((dataset['lag_histog_1']>0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_3 = \"nu_dias_histog_negativo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_3] = np.where((dataset['lag_histog_1']<0), 1, 0)\r\n",
        "\r\n",
        "    colname_nu_4 = \"nu_dias_histog_mismo_signo_%s\" % (ventana)\r\n",
        "    dataset[colname_nu_4] = np.where(((dataset['lag_histog_1']>0) & (dataset['histog']>0))|((dataset['lag_histog_1']<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "\r\n",
        "    dataset = dataset.drop(['lag_histog_1'], axis=1)\r\n",
        "    i = 2\r\n",
        "    while i < (ventana+1):\r\n",
        "      colname = \"lag_histog_%s\" % (i)\r\n",
        "      dataset[colname] = dataset.histog.shift(i)\r\n",
        "      dataset[colname_nu_1] = dataset[colname_nu_1] + np.where((dataset[colname] < (dataset.histog * 1.50)) & (dataset[colname] > (dataset.histog * 0.50)), 1, 0)\r\n",
        "      dataset[colname_nu_2] = dataset[colname_nu_2] + np.where((dataset[colname]>0), 1, 0)\r\n",
        "      dataset[colname_nu_3] = dataset[colname_nu_3] + np.where((dataset[colname]<0), 1, 0)\r\n",
        "      dataset[colname_nu_4] = dataset[colname_nu_4] + np.where(((dataset[colname]>0) & (dataset['histog']>0))|((dataset[colname]<0) & (dataset['histog']<0)), 1, 0)\r\n",
        "      i = i + 1\r\n",
        "      dataset = dataset.drop([colname], axis=1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "def calcula_AT_tendencias(dataset,lags):\r\n",
        "  print(\"Empiezo AT\", lags)\r\n",
        "  dataset_aux = dataset.copy()\r\n",
        "\r\n",
        "  # Construye las columnas para determinar si es un pico\r\n",
        "  i = 1\r\n",
        "  while i < (lags+1):\r\n",
        "      colname = 'p%sb' % (i)                                                  \r\n",
        "      dataset_aux[colname] = round(dataset_aux.y.shift(i),2)\r\n",
        "      j = i * -1\r\n",
        "      colname = 'p%sf' % (-j)                                                  \r\n",
        "      dataset_aux[colname] = round(dataset_aux.y.shift(j),2)\r\n",
        "      i = i + 1\r\n",
        "\r\n",
        "  # Determina si es un pico  \r\n",
        "  dataset_aux['maxb'] = round(dataset_aux.filter(regex=(\".*b\")).max(axis=1),2)\r\n",
        "  dataset_aux['maxf']= round(dataset_aux.filter(regex=(\".*f\")).max(axis=1),2)\r\n",
        "  dataset_aux['minb'] = round(dataset_aux.filter(regex=(\".*b\")).min(axis=1),2)\r\n",
        "  dataset_aux['minf'] = round(dataset_aux.filter(regex=(\".*f\")).min(axis=1),2)\r\n",
        "  dataset_aux['T'] = np.where((dataset_aux['y']>dataset_aux['maxb']) & (dataset_aux['y']>dataset_aux['maxf']) & ((pd.to_datetime(date.today()) - dataset_aux.fc).dt.days < lags*75), 1, 0) # Le agrego esta condición por performance, solo revisa picos relativos a fechas recientes segun los lagos (lag de 4, 300 días; lag de 8, 600)\r\n",
        "  dataset_aux['P'] = np.where((dataset_aux['y']<dataset_aux['minb']) & (dataset_aux['y']<dataset_aux['minf']) & ((pd.to_datetime(date.today()) - dataset_aux.fc).dt.days < lags*75), 1, 0) # \r\n",
        "\r\n",
        "  techos = dataset_aux[(dataset_aux['T']==1)]\r\n",
        "  techos['m'] = (techos.y.shift(1) - techos.y)/(techos.fc.shift(1) - techos.fc).dt.days\r\n",
        "  techos.name = 'techos'\r\n",
        "  pisos = dataset_aux[(dataset_aux['P']==1)]\r\n",
        "  pisos['m'] = (pisos.y.shift(1) - pisos.y)/(pisos.fc.shift(1) - pisos.fc).dt.days\r\n",
        "  pisos.name = 'pisos'\r\n",
        "  dataset_aux_list = [techos, pisos]\r\n",
        "#  print(\"Detecté picos\")\r\n",
        "\r\n",
        "  for dataset_aux_picos in dataset_aux_list:  # En cada dataset_aux (techos y pisos)\r\n",
        "#    print(\"Voy a desarrollar picos\")\r\n",
        "    name = dataset_aux_picos.name\r\n",
        "    dias = len(dataset_aux)\r\n",
        "    for index, row in dataset_aux_picos.iloc[1:].iterrows(): # Para cada pico detectado (fila del dataset_aux) a partir del segundo (porque el primero no tiene anterior, no tiene tendencia)\r\n",
        "#      print(\"Voy a desarrollar un pico\")\r\n",
        "      y_start = row['y']\r\n",
        "      pendiente = row['m']\r\n",
        "      if (dias < np.where(dataset_aux.fc==row['fc'])[0] + lags):\r\n",
        "        continue    \r\n",
        "      serie = [] # Crea la serie que va a contener el precio proyectado\r\n",
        "      serie = np.append(serie, np.repeat(np.nan, (np.where(dataset_aux.fc==row['fc'])[0] + lags))) # Appendea nulos hasta el día en el que confirmamos que nació una tendencia\r\n",
        "      i = np.where(dataset_aux.fc==row['fc'])[0] + lags\r\n",
        "      while (i < dias):\r\n",
        "        dia = i - (np.where(dataset_aux.fc==row['fc'])[0] + lags)\r\n",
        "        serie = np.append(serie, (y_start + pendiente*lags) + pendiente*dia)\r\n",
        "        i = i + 1 # Appendea el precio proyectado hasta el final\r\n",
        "\r\n",
        "      colname = '%s_%s_proy' % (name, index)  # Precio proyectado\r\n",
        "      dataset_aux[colname] = serie # Construye la columna de toda la serie\r\n",
        "#      print(\"Voy a calcular pruebas y pasajes\")\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue superado\r\n",
        "      colname_pass = '%s_%s_pass' % (name, index) # Pico pasado\r\n",
        "      if name == 'techos':\r\n",
        "        dataset_aux[colname_pass] = np.where(dataset_aux['y']>(dataset_aux[colname])*1.005, 1, 0)\r\n",
        "      elif name == 'pisos':\r\n",
        "        dataset_aux[colname_pass] = np.where(dataset_aux['y']<(dataset_aux[colname])*0.995, 1, 0)\r\n",
        "      dataset_aux[colname_pass] = dataset_aux[colname_pass].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con veces en la que el pico fue probado\r\n",
        "      colname_prueba = '%s_%s_prueba' % (name, index)  \r\n",
        "      dataset_aux[colname_prueba] = np.where((dataset_aux['y']>dataset_aux[colname]*0.995)&(dataset_aux['y']<dataset_aux[colname]*1.005), 1, 0)\r\n",
        "      dataset_aux[colname_prueba] = dataset_aux[colname_prueba].cumsum()\r\n",
        "\r\n",
        "      # Construyo columna con pendiente del pico\r\n",
        "      colname_pendiente = '%s_%s_pendiente' % (name, index)  \r\n",
        "      dataset_aux[colname_pendiente] = row['m']\r\n",
        "\r\n",
        "      # Creo la combinacion y elimino cada uno\r\n",
        "      colname_comb = '%s_%s' % (name, index)\r\n",
        "      dataset_aux[colname_comb] = dataset_aux[[colname, colname_pass, colname_prueba, colname_pendiente]].values.tolist()\r\n",
        "      del dataset_aux[colname]\r\n",
        "      del dataset_aux[colname_pass]\r\n",
        "      del dataset_aux[colname_prueba]\r\n",
        "      del dataset_aux[colname_pendiente]\r\n",
        "#      print(\"Desarrollé todo el pico\")\r\n",
        "\r\n",
        "  # Creo el objeto por cada techo o piso individual\r\n",
        "  names_techos = dataset_aux.filter(regex=(\"(techos)(.*)\")).columns\r\n",
        "  names_pisos = dataset_aux.filter(regex=(\"(pisos)(.*)\")).columns\r\n",
        "\r\n",
        "#  print(\"Voy a iterar sobre los precios (solo el último)\")\r\n",
        "  for index, row in dataset_aux.iterrows():  # Por cada fila del dataset_aux original (por cada precio)\r\n",
        "    if index != dataset_aux.index[-1]:\r\n",
        "#      print(\"No es el último, salteo\")\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "#      print(\"Es el último, genero variables\")\r\n",
        "\r\n",
        "      # Genero las rows vacías con las variables agregadas\r\n",
        "      nu_pruebas_techo_vivo_mas_probado = np.nan    \r\n",
        "      precio_proyectado_techo_vivo_mas_probado = np.nan\r\n",
        "      precio_proyectado_techo_vivo_mas_cercano = np.nan\r\n",
        "      precio_proyectado_techo_muerto_mas_cercano = np.nan\r\n",
        "      tendencia_techo_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "      nu_pruebas_piso_vivo_mas_probado = np.nan\r\n",
        "      precio_proyectado_piso_vivo_mas_probado = np.nan\r\n",
        "      precio_proyectado_piso_vivo_mas_cercano = np.nan\r\n",
        "      precio_proyectado_piso_muerto_mas_cercano = np.nan\r\n",
        "      tendencia_piso_vivo_mas_probado = np.nan\r\n",
        "\r\n",
        "      # Voy a recorrer cada tendencia proyectada para definir cuáles van, en caso de que corresponda lo asigno a estas variables agregadas\r\n",
        "\r\n",
        "      i = 0\r\n",
        "      while i < len(row.index): # Por cada uno de los picos de los que se puede armar tendencia\r\n",
        "        if (row.index[i] in names_techos):  # Si es un techo\r\n",
        "          if row[i][1]>5: # Si está muerto\r\n",
        "            if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_muerto_mas_cercano) or np.isnan(precio_proyectado_techo_muerto_mas_cercano): # Si está muerto y proyecta precio más cercano que el actual\r\n",
        "              precio_proyectado_techo_muerto_mas_cercano = row[i][0]\r\n",
        "              \r\n",
        "          else: # Si está vivo\r\n",
        "            if row[i][2] > nu_pruebas_techo_vivo_mas_probado or (np.isnan(nu_pruebas_techo_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "              nu_pruebas_techo_vivo_mas_probado = row[i][2]\r\n",
        "              precio_proyectado_techo_vivo_mas_probado = row[i][0]\r\n",
        "              tendencia_techo_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "            if (np.isnan(precio_proyectado_techo_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_techo_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "              precio_proyectado_techo_vivo_mas_cercano = row[i][0]\r\n",
        "\r\n",
        "        elif (row.index[i] in names_pisos):\r\n",
        "          if row[i][1]>5: # Si está muerto\r\n",
        "            if abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_muerto_mas_cercano) or np.isnan(precio_proyectado_piso_muerto_mas_cercano): # Si proyecta precio más cercano que el actual\r\n",
        "              precio_proyectado_piso_muerto_mas_cercano = row[i][0]\r\n",
        "              \r\n",
        "          else: # Si está vivo\r\n",
        "            if row[i][2] > nu_pruebas_piso_vivo_mas_probado or (np.isnan(nu_pruebas_piso_vivo_mas_probado) and row[i][2]>0): # Si fue más probado que el actual\r\n",
        "              nu_pruebas_piso_vivo_mas_probado = row[i][2]\r\n",
        "              precio_proyectado_piso_vivo_mas_probado = row[i][0]\r\n",
        "              tendencia_piso_vivo_mas_probado = row[i][3]\r\n",
        "\r\n",
        "            if (np.isnan(precio_proyectado_piso_vivo_mas_cercano)) or (abs(row['y']-row[i][0]) < abs(row['y']-precio_proyectado_piso_vivo_mas_cercano)): # Si, sin haber muerto, proyecta un techo más alto que el actual\r\n",
        "              precio_proyectado_piso_vivo_mas_cercano = row[i][0]\r\n",
        "        i = i + 1\r\n",
        "        \r\n",
        "    dataset.loc[index,'nu_pruebas_techo_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_techo_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_techo_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_techo_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_techo_vivo_mas_probado_'f\"{lags}\"] = tendencia_techo_vivo_mas_probado/row['y']\r\n",
        "\r\n",
        "    dataset.loc[index,'nu_pruebas_piso_vivo_mas_probado_'f\"{lags}\"] = nu_pruebas_piso_vivo_mas_probado\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_probado_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_probado - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_vivo_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_vivo_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'precio_proyectado_piso_muerto_mas_cercano_'f\"{lags}\"] = (precio_proyectado_piso_muerto_mas_cercano - row['y'])/row['y']\r\n",
        "    dataset.loc[index,'tendencia_piso_vivo_mas_probado_'f\"{lags}\"] = tendencia_piso_vivo_mas_probado/row['y']\r\n",
        "    #print(\"Iteré sobre los precios y construí variables\")\r\n",
        "\r\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3MBSDRVWKUS"
      },
      "source": [
        "today = date.today()\r\n",
        "fc_empieza = today + timedelta(days=-5000)\r\n",
        "fc_termina = today + timedelta(days=2)\r\n",
        "\r\n",
        "mvl = yf.download('^MERV', start=fc_empieza, end=fc_termina)\r\n",
        "print(\"Descargado Merval\")\r\n",
        "mvl = mvl[['Close']]\r\n",
        "mvl.reset_index(level=0, inplace=True)\r\n",
        "mvl.columns=['fc','mvl']\r\n",
        "base = pd.DataFrame()\r\n",
        "\r\n",
        "for ticker in (\r\n",
        "    'GGAL.BA',\r\n",
        "    'BMA.BA',\r\n",
        "    'BYMA.BA',\r\n",
        "    'CEPU.BA',\r\n",
        "    'COME.BA',\r\n",
        "    'CRES.BA',\r\n",
        "    'CVH.BA',\r\n",
        "    'EDN.BA',\r\n",
        "    'MIRG.BA',\r\n",
        "    'PAMP.BA',\r\n",
        "    'SUPV.BA',\r\n",
        "    'TECO2.BA',\r\n",
        "    'TGNO4.BA',\r\n",
        "    'TGSU2.BA',\r\n",
        "    'TRAN.BA',\r\n",
        "    'YPFD.BA'\r\n",
        "):\r\n",
        "  df = descarga(ticker, fc_empieza, fc_termina) # (Días empieza, días termina)\r\n",
        "  print(\"Descargado \", ticker)\r\n",
        "  \r\n",
        "  df = calcula_pc_merval(df)\r\n",
        "  df = calcula_amplitud(df)\r\n",
        "  df = estandariza_volumen(df)\r\n",
        " \r\n",
        "  df = calcula_medias(df)\r\n",
        "  df = calcula_historia(df, 5) # (Lags)\r\n",
        "  df = calcula_canalidad_y(df)\r\n",
        "  df = calcula_canalidad_histog_macd(df)\r\n",
        "\r\n",
        "  for per in (360, 120, 90, 60, 30, 15, 8, 4):\r\n",
        "    df = calcula_AT_tendencias(df,per)\r\n",
        "    print(\"Calculé AT para\", ticker, \"en lags de\", per)\r\n",
        "  \r\n",
        "  df = df.tail(1)\r\n",
        "  base = base.append(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "idhK9HlWXeD0",
        "outputId": "36834681-e13f-4db1-915a-c88f2f3169ea"
      },
      "source": [
        "mLink = 'https://github.com/fedefliguer/trading/blob/master/v1/v1_model.dat?raw=true'\r\n",
        "mfile = BytesIO(requests.get(mLink).content)\r\n",
        "model = pickle.load(mfile)\r\n",
        "X=base.iloc[:, 6:]\r\n",
        "preds = model.predict_proba(X)[:,1]\r\n",
        "prediccions = base.iloc[:, 0:3]\r\n",
        "prediccions['pred'] = preds\r\n",
        "prediccions.sort_values(by=['pred'], ascending=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fc</th>\n",
              "      <th>ticker</th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3352</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>TRAN.BA</td>\n",
              "      <td>26.700001</td>\n",
              "      <td>0.668461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>PAMP.BA</td>\n",
              "      <td>79.500000</td>\n",
              "      <td>0.620433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3356</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>CEPU.BA</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.606689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>EDN.BA</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.599605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>TGNO4.BA</td>\n",
              "      <td>42.250000</td>\n",
              "      <td>0.566848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>COME.BA</td>\n",
              "      <td>2.440000</td>\n",
              "      <td>0.448574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>CRES.BA</td>\n",
              "      <td>74.800003</td>\n",
              "      <td>0.392690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>BMA.BA</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>0.387761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>TGSU2.BA</td>\n",
              "      <td>152.699997</td>\n",
              "      <td>0.384938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>BYMA.BA</td>\n",
              "      <td>602.000000</td>\n",
              "      <td>0.359412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3352</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>TECO2.BA</td>\n",
              "      <td>204.449997</td>\n",
              "      <td>0.312707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>GGAL.BA</td>\n",
              "      <td>117.699997</td>\n",
              "      <td>0.286472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1147</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>SUPV.BA</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>0.277448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>CVH.BA</td>\n",
              "      <td>353.000000</td>\n",
              "      <td>0.237187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3353</th>\n",
              "      <td>2021-02-01</td>\n",
              "      <td>MIRG.BA</td>\n",
              "      <td>1578.500000</td>\n",
              "      <td>0.149284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             fc    ticker            y      pred\n",
              "3352 2021-02-01   TRAN.BA    26.700001  0.668461\n",
              "3353 2021-02-01   PAMP.BA    79.500000  0.620433\n",
              "3356 2021-02-01   CEPU.BA    35.000000  0.606689\n",
              "3353 2021-02-01    EDN.BA    26.000000  0.599605\n",
              "3353 2021-02-01  TGNO4.BA    42.250000  0.566848\n",
              "3353 2021-02-01   COME.BA     2.440000  0.448574\n",
              "3353 2021-02-01   CRES.BA    74.800003  0.392690\n",
              "3353 2021-02-01    BMA.BA   209.000000  0.387761\n",
              "3353 2021-02-01  TGSU2.BA   152.699997  0.384938\n",
              "876  2021-02-01   BYMA.BA   602.000000  0.359412\n",
              "3352 2021-02-01  TECO2.BA   204.449997  0.312707\n",
              "3353 2021-02-01   GGAL.BA   117.699997  0.286472\n",
              "1147 2021-02-01   SUPV.BA    55.000000  0.277448\n",
              "830  2021-02-01    CVH.BA   353.000000  0.237187\n",
              "3353 2021-02-01   MIRG.BA  1578.500000  0.149284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}